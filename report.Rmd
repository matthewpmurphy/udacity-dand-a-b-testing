---
title: "Design an A/B Test"
author: "Matthew Murphy"
date: "3/25/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Overview

>The hypothesis behind the experiment was that this might set clearer expectations to the students, reducing the amount of frustrated students who left the free trial because they did not have enough time and at the same time not reducing significantly the number of students who stay enrolled and complete the course. If the hypothesis turns out to be true, then the overall student experience could improve because coaches would be able to better support the students which will complete the course.

## Experiment Design

### Metric Choice

#### Invariant metrics

Invariant metrics should be independent of the experiment.  These metrics should not change between the different groups, experimental and control.  They provide a way to verify the integrity of our design.  The following metrics were chosen because of this.  Cookies, clicks, and click-through-probablity should all be the same throughout the experiment.  They occur prior to the screener pop-up, so it will have no effect.  This is also why these metrics would make poor evaluation metrics.  They will remain unchanged across the experiment.

  1. **Number of cookies**

  2. **Number of clicks**

  3. **Click-through-probability**

#### Evaluation metrics

Evaluation metrics are expected to change over the course of the experiment.  These changes allow us to measure the impact of the screener and test our hypothesis.  Because these metrics change, they make poor invariant metrics.  Both of these metrics occur after engaging the "start free trial" button which will allow us to measure the effect of the experiment screener on enrolling.

  1. **Gross conversion**: Gross conversion tells us the percentage of students that enroll on the course after clicking the "Start free trial button". 

  2. **Net conversion**: Net conversion tells us which students stay enrolled and end up making at least one payment. 

If our hypothesis turns out to be true, we expect the gross conversion to decrease, as students deterred by the screener are unlikely to make a payment and complete the course.  We also would like to see net conversion stay the same or increase.  To launch, we need to see both of these conditions met.

#### Unused metrics:

  1. **Number of user-ids**: This is not a good invariant metric because it is expected to change during the experiment.  It could be used as an evaluation metric, but it's not normalized and it's also already a factor in gross conversion.

  2. **Retention**: This is not invariant because we would expect it to change.  It's not a good evaluation metric because the magnitude of change is dependent on the variant number of user-ids (which again is related to gross conversion). It is also basically net conversion divided by gross conversion, and does not provide any meaningful information.


### Measuring Standard Deviation

Let's calculate the standard deviation for gross conversion and net conversion.  Since both are both probabilities, we can assume a binomial distribution, which will take on a normal distribution for a large enough sample size.

The baseline values can be [found here](https://docs.google.com/spreadsheets/d/1MYNUtC47Pg8hdoCjOXaHqF-thheGpUshrFA21BAJnNc/edit#gid=0).

For a sample size of 5000 cookies visiting the course overview page, the number of cookies we expect to click on the "Start Free Trial" would be:

```
N = 5000 * 3200/40000
N = 400
```

Then, the Standard Errors for 5000 pageviews will be:

```
Gross Conversion -> 0.0202
Net Conversion ->   0.0156
```

Gross conversion and net conversion both have cookies as the unit of analysis.  The analytic variability is likely to match the empirical variablity since the unit of analysis and unit of diversion are the same.  When those units differ, it may cause higher empirical variance due to correlation.

### Sizing

#### Number of Samples vs Power

With multiple tests being done, the chance of a rare event in the confidence interval increases. The Bonferroni correction compensates for that increase, but at the cost of increasing the probability of producing false positives. It can also be conservative if there are a large number of tests and/or the test statistics are positively correlated. Thus, I have elected not to use it for this experiment.

I used this [online calculator](http://www.evanmiller.org/ab-testing/sample-size.html) to calculate how large our sample size needs to be:

Gross Conversion:
```
    Base conversion rate = 20.625%
    Practical significance boundary = 1%
    Sample size: 25,835
```

Net Conversion:
```
    Base conversion rate = 10.93125%
    Practical significance boundary = 0.75%
    Sample size: 27,413
```

The sample size for net conversion is larger than gross conversion, so we will use that.  The click-through-probability on the "Start Free Trial" button is `0.08. We need to calculate the total number of pageviews per group, and then double that to get the total pageviews for both groups (control and experiment):

**Pageviews (per group)**
```
 27413 / 0.08  = 342,662.5
```

**Total pageviews**
```
342662.5 * 2 = 685,325
```

#### Duration vs Exposure

With 40,000 unique cookies that view the course overview page each day, I would divert 75% of that traffic (30,000 cookies) towards the experiment. Our A/B Test needs a total of 685,325 pageviews, and does not pose much risk.  This would mean the experiment will last roughly 23 days.

This is a non-risk experiment because it offers no physical risk to our users and does not include sensitive data.  Computer use is not generally considered a physical activity, and the hours per week only adds extra (2) mouse clicks.  Also, it is only asking for how many hours per week the user is committed to using on the course which is not considered to be "sensitive" informatin.


## Experiment Analysis

### Sanity Checks

Let's see if our invariant metrics are unchanged for both the control and experiment groups. 

Number of cookies:
```
    Total control pageviews : 345543
    Total experiment pageviews : 344660
    Total pageviews : 690203
    Expected probability : 0.5
```

Number of clicks:
```
    Total control clicks : 28378
    Total experiment clicks : 28325
    Total clicks : 56703
    Expected probability : 0.5
```

Click-through-probability:
```
    Observed (control) : 0.082126
    Observed (experiment) : 0.082182
    Pooled : 0.082154
```

Now let's check the confidence intervals (95%)

Number of cookies:
```
    Confidence interval : [0.4988, 0.5012]
    Observed value : 0.5006
    Passed? : YES
```

Number of clicks:
```
    Confidence interval : [0.4959, 0.5041]
    Observed value : 0.5005
    Passed? : YES
```

Click-through-probability:
```
    Confidence interval (of diff.) : [-0.001296, 0.001296]
    Observed difference : 0.00005663
    Passed? : YES
```

All three Sanity Checks have passed.

### Result Analysis

#### Effect Size Test

The sanity checks passed, but we need to determine if our evaluation metrics are statistically and practically significant.

Gross Conversion:
```
    Total clicks (control) : 17293
    Total clicks (experiment) : 17260
    Total enrollments (control) :  3785
    Total enrollments (experiment) :  3423
```

Net Conversion:
```
    Total clicks (control).......... 17293
    Total clicks (experiment)....... 17260
    Total payments (control)........  2033
    Total payments (experiment).....  1945
```

We will not evaluate confidence intervals (95%)

Gross Conversion:
```
    Control rate : 0.2189
    Experiment Rate : 0.1983
    Rate diff. (exp. - control) : -0.02055
    Pooled rate : 0.2086
    Pooled standard deviation : 0.004372
    
    Confidence interval : [-0.02912, -0.01199]
    Practical significance boundary : 0.01
    
    Statistically significant? YES
    Practically significant?  YES
```

Net Conversion:
```
    Control Rate : 0.1176
    Experiment Rate : 0.1127
    Rate diff. (exp. - control) : -0.004874
    Pooled rate : 0.1151
    Pooled standard deviation : 0.003434
    
    Confidence interval : [-0.01160, 0.001857]
    Practical significance boundary : 0.0075
    
    Statistically significant? NO 
    Practically significant? NO
```

For Gross Conversion, we see a statistically significant and practically significant decrease, which is what we wanted. Zero is not a part of the confidence interval, and the upper bound is past the practical significance boundary. So Gross Conversion has PASSED.

For Net Conversion, we wanted to see an increase or staying relatively the same. The calculations show that there is no statistical significant difference in net conversion, however the lower bound of the 95% confidence interval is past the boundary for practical significance. This means that any interpretation of the data will be uncertain, and we do not have enough statistical power to draw a strong conclusion.

#### Sign Tests

Gross Conversion:
```
    Number of successes.............  4
    Number of trials................ 23
    Probability..................... 0.5
    Two-tailed p-value.............. 0.0026
    Statistically significant?...... YES
```

Net Conversion:
```
    Number of successes............. 10
    Number of trials................ 23
    Probability..................... 0.5
    Two-tailed p-value.............. 0.6776
    Statistically significant?...... NO
```

#### Summary

The Bonferroni correction was not used because both of the evaluation metrics need to pass in order to launch. The Bonferroni correction is useful in reducing Type I errors, where any metric can pass in order to launch. It is not as effective in reducing Type II errors, where all parameters need to pass to launch.  This is the scenario we are in.

The results of both our hypothesis tests and sign tests align. Gross conversion has showing to be statistically significant, but net conversion is not.   Both tests also show that gross conversion is practically significant, but net conversion is not, which is due to the lower bound of the 95% confidence interval being pas the boundary for practical significance.

### Recommendation

Looking back at the hypothesis:

>The hypothesis behind the experiment was that this might set clearer expectations to the students, reducing the amount of frustrated students who left the free trial because they did not have enough time and at the same time not reducing significantly the number of students who stay enrolled and complete the course. If the hypothesis turns out to be true, then the overall student experience could improve because coaches would be able to better support the students which will complete the course.

The gross conversion rate was reduced, but net conversion stayed roughly the same.  It did reduce the amount of frustrated students which means that goal was achieved.  Another goal was to keep the net conversion about the same (or better).  However, net conversion was practically insignificant in this experiment, making the results inconclusive.  I would recommend **not to launch**. and instead conduct further experiments and gain more data about the  net conversion rate.


## Follow-Up Experiment

For a follow-up, I would try reducing the number of frustrated students by increasing communications with students in the free trial period.  I would look to send an email after the first week that checks in with the student and attempts to guage progress as well as offering tips and advice.  

This is a no-risk experiment.  There is no physical physical risk to the user as it is only email.  It also will not ask for any sensitive information from the user.  Email addresses are already provided at sign up.  I would include 100% of the (non-opted out) traffic in this experiment.  Users should be able to opt out if they don't want the emails, so that is not a concern.  

The baseline conversion rate is 53%.  With a practical significance boundary of 1%, that comes to a sample size at 39115.  With 660 users per day, it would take 60 days to get the necessary sample.

I would advise against running another experiment while this one is in place.  It may be difficult to discern what impact each of the experiments are having, and thus will not give us a clear picture of how effective each one may be.


**Hypothesis**: If students are encouraged, offered support and present options they will become more engaged and work through any difficulties.  We should then see retention rise.  If retention does rise by a significant amount, we can launch this new feature.

**Unit of Diversion**: User-Id. This change only impacts those who have enrolled in the program and subsequently made an account.

**Invariant Metrics**: Number of User-Ids. This should not change over both the control and experiment groups as there is nothing that is affecting their enrollment into the program.

**Evaluation Metrics**: 
* Retention: The goal for this experiment would be to increase this number.

## References
* https://en.wikipedia.org/wiki/Bonferroni_correction
* https://www.symbolab.com/solver/standard-deviation-calculator
* https://www.evanmiller.org/ab-testing/sample-size.html
* https://www.graphpad.com/quickcalcs/pvalue1.cfm
* https://medium.com/@jinfenghuang/a-b-testing-8e349afeb370

